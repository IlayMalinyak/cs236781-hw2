{
  "config": {
    "run_name": "exp1_1",
    "out_dir": "./results",
    "seed": 42,
    "device": null,
    "bs_train": 128,
    "bs_test": 32,
    "batches": 100,
    "epochs": 100,
    "early_stopping": 5,
    "checkpoints": null,
    "lr": 0.003007970289449718,
    "reg": 0.008128033924872312,
    "filters_per_layer": [
      64
    ],
    "layers_per_block": 2,
    "pool_every": 1,
    "hidden_dims": [
      512,
      512
    ],
    "model_type": "cnn",
    "activation_type": "relu",
    "activation_params": {},
    "pooling_type": "max",
    "pooling_params": {
      "kernel_size": 2
    },
    "batchnorm": true,
    "dropout": 0.1633292046324038,
    "bottleneck": false,
    "loss_fn": "cross entropy",
    "optimizer": "Adam",
    "hp_optim": {
      "betas": [
        0.9969682475071221,
        0.9835809796844466
      ]
    },
    "subset": false
  },
  "results": {
    "num_epochs": 15,
    "train_loss": [
      1.7561574512735352,
      1.3336100797823933,
      1.1822337936562346,
      1.0665062949480608,
      0.9913904983979052,
      0.9284608013489667,
      0.9063304411175915,
      0.903903288761978,
      0.9001828310129892,
      0.8798553227158763,
      0.866459976681663,
      0.8622815421475168,
      0.8702543775748719,
      0.8732747667280915,
      0.8675502355751175
    ],
    "train_acc": [
      34.586,
      52.016,
      57.854,
      61.85,
      64.74,
      67.304,
      68.052,
      68.186,
      68.264,
      69.248,
      69.392,
      69.746,
      69.318,
      69.374,
      69.538
    ],
    "test_loss": [
      1.4323651124113284,
      1.247108373207787,
      1.141872786294919,
      1.0578275812319673,
      1.0128111014731775,
      1.020401725182518,
      1.0055114201273019,
      1.015138794914983,
      0.9868207046399101,
      0.9552909886113371,
      0.9657456910076995,
      0.9803578425139284,
      0.9745919715863066,
      0.9765108256294324,
      0.9769964178149312
    ],
    "test_acc": [
      47.72,
      55.39,
      59.49,
      62.31,
      63.86,
      64.2,
      65.29,
      64.64,
      65.58,
      66.28,
      65.94,
      65.26,
      65.89,
      65.59,
      66.06
    ]
  }
}